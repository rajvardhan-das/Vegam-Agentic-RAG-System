<<<<<<< HEAD
# Agentic RAG Chatbot ðŸš€

RAG | MCP | Milvus | Self-Hosted LLM | Streamlit | Multi Agent | Error Handling | FastAPI | Gemini

A full-stack **Agentic Retrieval-Augmented Generation (RAG)** system built with a modular, production-oriented architecture.  
The system supports **hybrid retrieval**, **self-hosted LLMs**, **Milvus vector database** and **MCP-style tool servers**.

---

## âœ¨ Key Features

- ðŸ§  **Agentic Architecture**
  - Planner Agent
  - Retriever Agent (Hybrid Retrieval)
  - Context Builder Agent
  - Answer Agent
  - Orchestrator

- ðŸ” **Advanced Retrieval Techniques**
  - Dense vector retrieval (embeddings)
  - Keyword-based lexical search
  - Hybrid fusion (dense + keyword)
  - Embedding-based reranking

- ðŸ—„ï¸ **Vector Database**
  - Milvus (Docker-based)
  - Swappable vectorstore design

- ðŸ¤– **LLM Support**
  - Primary: **Ollama (LLaMA 3)**
  - Secondary: **Gemini API**
  - Runtime LLM switching from frontend

- ðŸ”Œ **MCP (Model Context Protocol) Server**
  - Exposes ingestion and query as callable tools
  - Designed for agent-to-agent or tool-based invocation

- ðŸŒ **FastAPI Backend**
  - File upload & ingestion
  - Chat endpoint
  - LLM selection endpoint

- ðŸŽ¨ **Streamlit Frontend**
  - Theme with lavender accents
  - Glassmorphism (blurred panels)
  - Animated chat messages
  - Rounded, modern UI

- ðŸ›¡ï¸ **Comprehensive Error Handling**
  - Centralized custom exceptions
  - Structured logging
  - Graceful API failures

---

## âž• Extra Implementation: Hybrid Runtime LLM Switching

The system supports **runtime switching between multiple LLM providers** without restarting the application.

Currently supported:

- Ollama (self-hosted LLaMA 3)
- Gemini API

Users can select the desired LLM directly from the frontend UI, and the backend dynamically routes all generation requests to the chosen provider.

This enables:

- Easy benchmarking across models  
- Cost vs quality trade-offs  
- Fault tolerance (fallback to alternate LLM)  
- True provider-agnostic architecture  

Implementation details:

- Centralized LLM router (`services/llm.py`)
- Active provider stored in configuration
- FastAPI endpoint: `/set-llm`
- Frontend dropdown for live switching

This hybrid LLM routing layer is implemented in addition to the assignment requirements.

---


## ðŸ–¼ï¸ Sample Screenshot

Below is an example interaction with the Agentic RAG Chatbot showing document-grounded answering:

![Sample Chat Response](sample.png)


> Additional UI and system screenshots are provided in the accompanying PDF document.


---

## ðŸ—ï¸ System Architecture

Streamlit UI
â†“
FastAPI Backend
â†“
Orchestrator Agent
â”œâ”€â”€ Planner Agent
â”œâ”€â”€ Retriever Agent
â”‚ â”œâ”€â”€ Dense Search
â”‚ â”œâ”€â”€ Keyword Search
â”‚ â””â”€â”€ Reranker
â”œâ”€â”€ Context Builder
â””â”€â”€ Answer Agent
â†“
LLM (Ollama / Gemini)
â†“
Milvus Vector DB


---

## ðŸ“ Project Structure


Agentic-RAG-Rajvardhan/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ agents/
â”‚ â”‚ â”œâ”€â”€ planner.py
â”‚ â”‚ â”œâ”€â”€ retriever.py
â”‚ â”‚ â”œâ”€â”€ context_builder.py
â”‚ â”‚ â”œâ”€â”€ answer_agent.py
â”‚ â”‚ â””â”€â”€ orchestrator.py
â”‚ â”œâ”€â”€ ingest/
â”‚ â”‚ â”œâ”€â”€ loader.py
â”‚ â”‚ â”œâ”€â”€ cleaner.py
â”‚ â”‚ â”œâ”€â”€ chunker.py
â”‚ â”‚ â””â”€â”€ pipeline.py
â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â”œâ”€â”€ llm.py
â”‚ â”‚ â”œâ”€â”€ embeddings.py
â”‚ â”‚ â”œâ”€â”€ reranker.py
â”‚ â”‚ â””â”€â”€ keyword_search.py
â”‚ â”œâ”€â”€ vectorstore/
â”‚ â”‚ â””â”€â”€ db.py
â”‚ â”œâ”€â”€ exceptions/
â”‚ â”‚ â”œâ”€â”€ exceptions.py
â”‚ â”‚ â””â”€â”€ logger.py
â”‚ â”œâ”€â”€ app.py
â”‚ â””â”€â”€ mcp_server.py
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ streamlit_app.py
â”œâ”€â”€ data/
â”‚ â””â”€â”€ sample_docs/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone Repository & Create Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate   # Windows

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Start Milvus (Docker)
cd C:\milvus

Invoke-WebRequest https://raw.githubusercontent.com/milvus-io/milvus/refs/heads/master/scripts/standalone_embed.bat -OutFile standalone.bat

standalone.bat start


Verify:

docker ps

4ï¸âƒ£ Start Backend (LLM & FastAPI)
ollama serve
ollama list

python -m uvicorn backend.app:app --reload


Backend runs at:

http://localhost:8000


Swagger UI:

http://localhost:8000/docs


5ï¸âƒ£ Start MCP Server (Optional)
python -m uvicorn backend.mcp_server:app --port 9000 --reload


6ï¸âƒ£ Start Frontend (Streamlit)
streamlit run frontend/streamlit_app.py

------

ðŸ”Œ MCP Endpoints

Endpoint	    Description
/mcp/query	    Query the Agentic RAG system
/mcp/ingest	    Ingest documents into vector DB

-------

ðŸ§ª Example API Usage

Chat

POST /chat
{
  "query": "Where is Paris?"
}


Upload

POST /upload (multipart/form-data)



ðŸ† Assignment Coverage
Requirement	                    Status
Agentic RAG	                    âœ…
Milvus Vector Database	        âœ…
Advanced Retrieval Techniques	âœ…
Self-Hosted LLM	                âœ…
MCP Server	                    âœ…
Comprehensive Error Handling	âœ…
Frontend UI	                    âœ…

-------------

ðŸ“Œ Notes

Designed for extensibility & clarity

Vector DB layer is swappable

MCP implemented as tool-style server

UI customized beyond default Streamlit

ðŸ‘¤ Author

Rajvardhan
Data Scientist



=======
# Vegam-Agentic-RAG-System
Built an Agentic RAG System that uses AI agents to intelligently retrieve and  answer questions from documents.
>>>>>>> 1cb833824e68fa1b10e36e325ceca173ecbad8a1
